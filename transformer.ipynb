{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq模型(encoder + attention + decoder)\n",
    "* CNN\n",
    "    * 权重共享\n",
    "        * 平移不变性（ 据偏置归纳）\n",
    "        * 可并行计算\n",
    "    * 滑动窗口\n",
    "        * 局部关联性建模\n",
    "            * 依靠多层堆积来进行长程建模（感受野）\n",
    "    * 对相对位置敏感，对绝对位置不敏感\n",
    "* RNN：依次有序进行建模\n",
    "    * 对顺序敏感\n",
    "    * 串行计算耗时\n",
    "    * 长程建模能力弱\n",
    "    * 计算复杂度与序列长度呈线性关系\n",
    "    * 单步计算复杂度不变\n",
    "    * 对相对位置敏感，对绝对位置敏感\n",
    "* Transformer\n",
    "    * 无局部假设\n",
    "        * 可并行计算\n",
    "        * 对相对位置不敏感\n",
    "    * 无有序假设\n",
    "        * 需要位置编码来反映位置变化对于特征的影响：Positional Embedding\n",
    "        * 对绝对位置不敏感\n",
    "    * 任意两字符都可以建模\n",
    "        * 擅长长短程建模\n",
    "        * 自注意力机制需要序列长度的平方级别复杂度（缺点）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer结构\n",
    "\n",
    "#### Encoder:\n",
    "* input word embedding\n",
    "    * 由稀疏的one-hot编码进入一个不带bias的FFN得到一个稠密的连续向量\n",
    "* positional encoding\n",
    "    * 通过sin/cos来固定表征\n",
    "        * 每个位置是确定性的\n",
    "        * 对于不同的句子，相同位置的距离一致\n",
    "        * 可以推广到更长的测试句子\n",
    "    * pe(pos+k)可以写成P(k)的线性组合\n",
    "    * 通过残差连接使得位置信息流入深层\n",
    "* multi-head self-attention\n",
    "    * 使得建模能力更强，表征空间更丰富\n",
    "    * 由多组Q,K,VB构成，每组单独计算一个attention向量\n",
    "    * 把每组的attention向量拼起来，并进入一个不带bias的FFN得到最终的向量\n",
    "* feed-forward network\n",
    "    * 只考虑每个单独位置进行建模\n",
    "    * 不同位置参数共享\n",
    "    * 类似于1*1的 pointwise convolution\n",
    "\n",
    "#### Decoder\n",
    "* output word embedding\n",
    "* multi-head self-attention\n",
    "* multi-hrad cross-attention\n",
    "* feed-forward network\n",
    "* softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer难点与实现\n",
    "* word embedding\n",
    "* position embedding\n",
    "* encoder self-attention mask\n",
    "* intra-attention mask\n",
    "* decoder self-attention mask\n",
    "* multi-head self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4], dtype=torch.int32) tensor([4, 4], dtype=torch.int32)\n",
      "tensor([[4, 4, 4, 6, 0],\n",
      "        [2, 6, 3, 7, 0]]) tensor([[5, 6, 7, 4, 0],\n",
      "        [1, 4, 6, 7, 0]])\n",
      "Parameter containing:\n",
      "tensor([[-0.3057, -1.1836, -0.0845, -0.4707, -1.9558, -0.6312, -1.6973, -0.8162],\n",
      "        [-1.6348, -0.5626, -0.3832,  1.1118, -0.6217, -0.3915, -0.0535, -0.3769],\n",
      "        [-0.4993,  0.3921, -1.3122,  1.0456, -0.9589,  0.4589,  0.1509, -0.5040],\n",
      "        [ 0.2234,  0.0507, -0.0209,  1.8107,  0.6105,  2.2234, -0.0382,  2.5241],\n",
      "        [-1.7896, -1.3420,  1.3553,  0.7506, -0.2260, -1.1905,  1.7992, -1.2508],\n",
      "        [ 1.2360, -1.3867, -0.3615, -1.8234, -0.8902,  0.1038,  0.2267,  0.3105],\n",
      "        [-0.1237, -0.2180,  1.2855,  0.7839, -0.5290,  1.0634, -0.5195, -1.9405],\n",
      "        [-0.9626,  0.1429,  1.3647, -0.1821, -0.5914,  1.5330,  1.2365, -0.6320],\n",
      "        [-0.3460,  0.4774, -0.5588, -0.1905, -3.0375, -1.6223, -1.5261,  0.2128]],\n",
      "       requires_grad=True)\n",
      "src_seq:  tensor([[4, 4, 4, 6, 0],\n",
      "        [2, 6, 3, 7, 0]])\n",
      "tgt_seq tensor([[5, 6, 7, 4, 0],\n",
      "        [1, 4, 6, 7, 0]])\n",
      "src_embedding tensor([[[-1.7896, -1.3420,  1.3553,  0.7506, -0.2260, -1.1905,  1.7992,\n",
      "          -1.2508],\n",
      "         [-1.7896, -1.3420,  1.3553,  0.7506, -0.2260, -1.1905,  1.7992,\n",
      "          -1.2508],\n",
      "         [-1.7896, -1.3420,  1.3553,  0.7506, -0.2260, -1.1905,  1.7992,\n",
      "          -1.2508],\n",
      "         [-0.1237, -0.2180,  1.2855,  0.7839, -0.5290,  1.0634, -0.5195,\n",
      "          -1.9405],\n",
      "         [-0.3057, -1.1836, -0.0845, -0.4707, -1.9558, -0.6312, -1.6973,\n",
      "          -0.8162]],\n",
      "\n",
      "        [[-0.4993,  0.3921, -1.3122,  1.0456, -0.9589,  0.4589,  0.1509,\n",
      "          -0.5040],\n",
      "         [-0.1237, -0.2180,  1.2855,  0.7839, -0.5290,  1.0634, -0.5195,\n",
      "          -1.9405],\n",
      "         [ 0.2234,  0.0507, -0.0209,  1.8107,  0.6105,  2.2234, -0.0382,\n",
      "           2.5241],\n",
      "         [-0.9626,  0.1429,  1.3647, -0.1821, -0.5914,  1.5330,  1.2365,\n",
      "          -0.6320],\n",
      "         [-0.3057, -1.1836, -0.0845, -0.4707, -1.9558, -0.6312, -1.6973,\n",
      "          -0.8162]]], grad_fn=<EmbeddingBackward0>) torch.Size([2, 5, 8])\n",
      "tgt_embedding tensor([[[ 1.3449, -1.0615,  1.3039, -1.0996, -0.5993, -1.9874,  1.6395,\n",
      "           0.5002],\n",
      "         [-0.0947, -0.8163,  0.3399,  0.5664,  1.0564,  0.2977,  0.9305,\n",
      "          -0.0946],\n",
      "         [-0.8919,  2.0158, -0.0429,  1.1663,  1.2684, -0.2965,  0.1599,\n",
      "           0.0477],\n",
      "         [-0.8643, -0.9006, -0.3224, -1.3309,  0.5963,  1.0003,  1.1956,\n",
      "          -0.1288],\n",
      "         [-0.6068,  0.0760,  0.0455,  0.1389, -0.9402,  1.0134, -0.9481,\n",
      "          -0.4908]],\n",
      "\n",
      "        [[-1.1603,  1.2253,  1.3027, -1.5617,  0.0194,  0.1242, -0.1120,\n",
      "           1.2471],\n",
      "         [-0.8643, -0.9006, -0.3224, -1.3309,  0.5963,  1.0003,  1.1956,\n",
      "          -0.1288],\n",
      "         [-0.0947, -0.8163,  0.3399,  0.5664,  1.0564,  0.2977,  0.9305,\n",
      "          -0.0946],\n",
      "         [-0.8919,  2.0158, -0.0429,  1.1663,  1.2684, -0.2965,  0.1599,\n",
      "           0.0477],\n",
      "         [-0.6068,  0.0760,  0.0455,  0.1389, -0.9402,  1.0134, -0.9481,\n",
      "          -0.4908]]], grad_fn=<EmbeddingBackward0>) torch.Size([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Word Embedding\n",
    "# 以序列建模为例\n",
    "# 考虑source sentence和target sentence\n",
    "\n",
    "# 构建序列，序列的字符以索引的形式表示\n",
    "# 源序列和目标序列的长度\n",
    "batch_size = 2\n",
    "# 单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "model_dim = 8 # 模型特征大小\n",
    "# 序列的最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "\n",
    "src_len = torch.randint(2, 5, (batch_size,)).to(torch.int32)\n",
    "tgt_len = torch.randint(2, 5, (batch_size,)).to(torch.int32)\n",
    "print(src_len, tgt_len)\n",
    "\n",
    "# 以单词索引构成的源句子和目标句子，构建batch\n",
    "# 需要pad成最大长度，向其中填充0\n",
    "src_seq = torch.cat([\n",
    "    torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len - L)), 0) \\\n",
    "        for L in src_len\n",
    "    ])\n",
    "tgt_seq = torch.cat([\n",
    "    torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len - L)), 0) \\\n",
    "        for L in tgt_len\n",
    "    ])\n",
    "print(src_seq, tgt_seq)\n",
    "\n",
    "# 构造word embedding\n",
    "src_embedding_table = nn.Embedding(max_num_src_words + 1, model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1, model_dim)\n",
    "\n",
    "print(src_embedding_table.weight)\n",
    "\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "print('src_seq: ', src_seq)\n",
    "print('tgt_seq', tgt_seq)\n",
    "print('src_embedding', src_embedding, src_embedding.shape)\n",
    "print('tgt_embedding', tgt_embedding, tgt_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[   1.,   10.,  100., 1000.]])\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01]])\n"
     ]
    }
   ],
   "source": [
    "# 构造position embedding\n",
    "max_position_len = 5\n",
    "pos_matrix = torch.arange(max_position_len).reshape((-1, 1))\n",
    "index_matrix = torch.pow(10000, torch.arange(0, 8, 2).reshape((1, -1)) / model_dim)\n",
    "pe_table = torch.zeros(max_position_len, model_dim)\n",
    "pe_table[:, 0::2] = torch.sin(pos_matrix / index_matrix) # 其余全部交给广播操作\n",
    "pe_table[:, 1::2] = torch.cos(pos_matrix / index_matrix)\n",
    "print(pos_matrix)\n",
    "print(index_matrix)\n",
    "print(pe_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01]]) torch.Size([5, 8])\n",
      "torch.Size([2, 5])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(pe\u001b[38;5;241m.\u001b[39mweight, pe\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(src_seq\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m src_pe \u001b[38;5;241m=\u001b[39m \u001b[43mpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_seq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "pe = nn.Embedding(max_position_len, model_dim)\n",
    "pe.weight = nn.Parameter(pe_table, requires_grad=False)\n",
    "\n",
    "src_position = [torch.arange(L) for L in src_len]\n",
    "\n",
    "print(pe.weight, pe.weight.shape)\n",
    "print(src_seq.shape)\n",
    "src_pe = pe(src_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# === Pytorch手写Transformer完整代码\n",
    "# ======================================\n",
    "# 数据构建\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# 这里是最开始试的时候使用的，之后任务改为机器翻译的任务\n",
    "#################################\n",
    "\n",
    "device = 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "# transformer epochs\n",
    "epochs = 100\n",
    "# epochs = 1000\n",
    "\n",
    "# 这里我没有用什么大型的数据集，而是手动输入了两对中文→英语的句子\n",
    "# 还有每个字的索引也是我手动硬编码上去的，主要是为了降低代码阅读难度\n",
    "# S: Symbol that shows Start of decoding input\n",
    "# E: Symbol that shows End of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "# 训练集\n",
    "sentences = [\n",
    "    # 中文和英语的单词个数不要求相同\n",
    "    # enc_input                dec_input           dec_output\n",
    "    ['我 有 一 个 好 朋 友 P', 'S I have a good friend .', 'I have a good friend . E'],\n",
    "    ['我 有 零 个 女 朋 友 P', 'S I have zero girl friend .', 'I have zero girl friend . E'],\n",
    "    ['我 有 一 个 男 朋 友 P', 'S I have a boy friend .', 'I have a boy friend . E']\n",
    "]\n",
    "\n",
    "# 测试集（希望transformer能达到的效果）\n",
    "# 输入：\"我 有 一 个 女 朋 友\"\n",
    "# 输出：\"i have a girlfriend\"\n",
    "\n",
    "# 中文和英语的单词要分开建立词库\n",
    "# Padding Should be Zero\n",
    "\n",
    "src_vocab = {'P': 0, '我': 1, '有': 2, '一': 3,\n",
    "             '个': 4, '好': 5, '朋': 6, '友': 7, '零': 8, '女': 9, '男': 10}\n",
    "src_idx2word = {i: w for i, w in enumerate(src_vocab)}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "tgt_vocab = {'P': 0, 'I': 1, 'have': 2, 'a': 3, 'good': 4,\n",
    "             'friend': 5, 'zero': 6, 'girl': 7,  'boy': 8, 'S': 9, 'E': 10, '.': 11}\n",
    "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "src_len = 8  # （源句子的长度）enc_input max sequence length\n",
    "tgt_len = 7  # dec_input(=dec_output) max sequence length\n",
    "\n",
    "# Transformer Parameters\n",
    "# d_model\n",
    "    # 是每一层Encoder和Decoder Layer的特征维度，一直保持一致\n",
    "    # 表示Transformer的语义空间维度，是模型捕捉语义信息的核心特征空间\n",
    "d_model = 512  # Embedding Size（token embedding和position编码的维度）\n",
    "\n",
    "# FeedForward dimension (两次线性层中的隐藏层 512->2048->512，线性层是用来做特征提取的）\n",
    "    # 增强特征表达能力\n",
    "        # 多头注意力关注的是输入序列中不同位置之间的关系，但是不足以捕捉每个位置的丰富特征\n",
    "        # FF Layer是使用更高维度的中间层（通常4 * d_model）对每个位置的特征进行非线性变换，从而提升表达能力\n",
    "    # 加强非线性能力\n",
    "        # 注意力机制本质上还是线性的，FFN引入非线性激活函数使模型更具有表达力\n",
    "    # 实现point-wise逐位置特征更新\n",
    "        # FFN对输入序列的每个位置单独计算\n",
    "# 当然最后会再接一个projection层，是模型输出部分（特别是Transformer Decoder输出后）的线性映射层\n",
    "    # 映射到词汇表大小\n",
    "    # 计算最终的概率分布，完成最终的序列预测任务\n",
    "    \n",
    "d_ff = 2048\n",
    "d_k = d_v = 64  # dimension of K(=Q), V（Q和K的维度需要相同，这里为了方便让K=V）\n",
    "n_layers = 6  # number of Encoder of Decoder Layer（Block的个数）\n",
    "n_heads = 8  # number of heads in Multi-Head Attention（有几套头）\n",
    "\n",
    "\n",
    "# ==============================================================================================\n",
    "# 数据构建\n",
    "\n",
    "def make_data(sentences):\n",
    "    \"\"\"把单词序列转换为数字序列\"\"\"\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for i in range(len(sentences)):\n",
    "        enc_input = [[src_vocab[n] for n in sentences[i][0].split()]]\n",
    "        dec_input = [[tgt_vocab[n] for n in sentences[i][1].split()]]\n",
    "        dec_output = [[tgt_vocab[n] for n in sentences[i][2].split()]]\n",
    "\n",
    "        #[[1, 2, 3, 4, 5, 6, 7, 0], [1, 2, 8, 4, 9, 6, 7, 0], [1, 2, 3, 4, 10, 6, 7, 0]]\n",
    "        enc_inputs.extend(enc_input) # 使用extend是因为enc_input等是两层的列表\n",
    "        # print(enc_inputs)\n",
    "        #[[9, 1, 2, 3, 4, 5, 11], [9, 1, 2, 6, 7, 5, 11], [9, 1, 2, 3, 8, 5, 11]]\n",
    "        dec_inputs.extend(dec_input)\n",
    "        #[[1, 2, 3, 4, 5, 11, 10], [1, 2, 6, 7, 5, 11, 10], [1, 2, 3, 8, 5, 11, 10]]\n",
    "        dec_outputs.extend(dec_output)\n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs) # longtensor主要特指int64\n",
    "\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
    "\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    \"\"\"自定义DataLoader\"\"\"\n",
    "\n",
    "    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "\n",
    "\n",
    "loader = Data.DataLoader(\n",
    "    MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "def build_vocab(sentences) -> Tuple[Dict[str, int], Dict[int, str], int]:\n",
    "    '''构建词汇表，返回最大长度'''\n",
    "    max_len = 1\n",
    "    word2idx = {'P': 0, 'S': 1, 'E': 2}\n",
    "    idx2word = {0: 'P', 2: 'S', 3: 'E'}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # 使用正则表达式分词，确保标点符号作为独立的词\n",
    "        words = re.findall(r'\\w+|[^\\s\\w]', sentence)\n",
    "        if len(words) > max_len:\n",
    "            max_len = len(words)\n",
    "        for word in words:\n",
    "            if word not in word2idx:\n",
    "                idx = len(word2idx)\n",
    "                word2idx[word] = idx\n",
    "                idx2word[idx] = word\n",
    "    \n",
    "    return word2idx, idx2word, max_len + 1\n",
    "\n",
    "def make_data(src_file, tgt_file, src_vocab, tgt_vocab, max_src_len, max_tgt_len):\n",
    "    \"\"\"从文件读取数据并转换为模型所需格式\"\"\"\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    with open(src_file, 'r', encoding='utf-8') as f_src, open(tgt_file, 'r', encoding='utf-8') as f_tgt:\n",
    "        for src_line, tgt_line in zip(f_src, f_tgt):\n",
    "            # 使用正则表达式分词，确保标点符号被单独处理\n",
    "            src_tokens = re.findall(r'\\w+|[^\\s\\w]', src_line.strip())\n",
    "            tgt_tokens = re.findall(r'\\w+|[^\\s\\w]', tgt_line.strip())\n",
    "\n",
    "            # 添加开始（S）和结束（E）符号\n",
    "            dec_input_tokens = ['S'] + tgt_tokens\n",
    "            dec_output_tokens = tgt_tokens + ['E']\n",
    "\n",
    "            # 截断或填充\n",
    "            src_seq = [src_vocab.get(word, 0) for word in src_tokens[:max_src_len]]\n",
    "            tgt_input_seq = [tgt_vocab.get(word, 0) for word in dec_input_tokens[:max_tgt_len]]\n",
    "            tgt_output_seq = [tgt_vocab.get(word, 0) for word in dec_output_tokens[:max_tgt_len]]\n",
    "\n",
    "            src_seq += [0] * (max_src_len - len(src_seq))  # Padding\n",
    "            tgt_input_seq += [0] * (max_tgt_len - len(tgt_input_seq))\n",
    "            tgt_output_seq += [0] * (max_tgt_len - len(tgt_output_seq))\n",
    "\n",
    "            enc_inputs.append(src_seq)\n",
    "            dec_inputs.append(tgt_input_seq)\n",
    "            dec_outputs.append(tgt_output_seq)\n",
    "            \n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
    "\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    \"\"\"自定义DataLoader\"\"\"\n",
    "\n",
    "    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = '/home/shenc/Desktop/wmt14/europarl-v7.de-en.de'\n",
    "tgt_file = '/home/shenc/Desktop/wmt14/europarl-v7.de-en.en'\n",
    "\n",
    "# 从文件中读数据\n",
    "with open(src_file, 'r', encoding='utf-8') as f_src, open(tgt_file, 'r', encoding='utf-8') as f_tgt:\n",
    "    src_sentences = f_src.readlines()\n",
    "    tgt_sentences = f_tgt.readlines()\n",
    "\n",
    "# 从这里获得最大序列长度，方便以后\n",
    "src_vocab, src_idx2word, max_len_src = build_vocab(src_sentences)\n",
    "tgt_vocab, tgt_idx2word, max_len_tgt = build_vocab(tgt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0,\n",
       " 'Wiederaufnahme': 1,\n",
       " 'der': 2,\n",
       " 'Sitzungsperiode': 3,\n",
       " 'Ich': 4,\n",
       " 'erkläre': 5,\n",
       " 'die': 6,\n",
       " 'am': 7,\n",
       " 'Freitag,': 8,\n",
       " 'dem': 9,\n",
       " '17.': 10,\n",
       " 'Dezember': 11,\n",
       " 'unterbrochene': 12,\n",
       " 'des': 13,\n",
       " 'Europäischen': 14,\n",
       " 'Parlaments': 15,\n",
       " 'für': 16,\n",
       " 'wiederaufgenommen,': 17,\n",
       " 'wünsche': 18,\n",
       " 'Ihnen': 19,\n",
       " 'nochmals': 20,\n",
       " 'alles': 21,\n",
       " 'Gute': 22,\n",
       " 'zum': 23,\n",
       " 'Jahreswechsel': 24,\n",
       " 'und': 25,\n",
       " 'hoffe,': 26,\n",
       " 'daß': 27,\n",
       " 'Sie': 28,\n",
       " 'schöne': 29,\n",
       " 'Ferien': 30,\n",
       " 'hatten.': 31,\n",
       " 'Wie': 32,\n",
       " 'feststellen': 33,\n",
       " 'konnten,': 34,\n",
       " 'ist': 35,\n",
       " 'gefürchtete': 36,\n",
       " '\"Millenium-Bug': 37,\n",
       " '\"': 38,\n",
       " 'nicht': 39,\n",
       " 'eingetreten.': 40,\n",
       " 'Doch': 41,\n",
       " 'sind': 42,\n",
       " 'Bürger': 43,\n",
       " 'einiger': 44,\n",
       " 'unserer': 45,\n",
       " 'Mitgliedstaaten': 46,\n",
       " 'Opfer': 47,\n",
       " 'von': 48,\n",
       " 'schrecklichen': 49,\n",
       " 'Naturkatastrophen': 50,\n",
       " 'geworden.': 51,\n",
       " 'Im': 52,\n",
       " 'Parlament': 53,\n",
       " 'besteht': 54,\n",
       " 'Wunsch': 55,\n",
       " 'nach': 56,\n",
       " 'einer': 57,\n",
       " 'Aussprache': 58,\n",
       " 'im': 59,\n",
       " 'Verlauf': 60,\n",
       " 'dieser': 61,\n",
       " 'in': 62,\n",
       " 'den': 63,\n",
       " 'nächsten': 64,\n",
       " 'Tagen.': 65,\n",
       " 'Heute': 66,\n",
       " 'möchte': 67,\n",
       " 'ich': 68,\n",
       " 'bitten': 69,\n",
       " '-': 70,\n",
       " 'das': 71,\n",
       " 'auch': 72,\n",
       " 'Kolleginnen': 73,\n",
       " 'Kollegen': 74,\n",
       " '-,': 75,\n",
       " 'allen': 76,\n",
       " 'Opfern': 77,\n",
       " 'Stürme,': 78,\n",
       " 'insbesondere': 79,\n",
       " 'verschiedenen': 80,\n",
       " 'Ländern': 81,\n",
       " 'Union,': 82,\n",
       " 'Schweigeminute': 83,\n",
       " 'zu': 84,\n",
       " 'gedenken.': 85,\n",
       " 'bitte': 86,\n",
       " 'Sie,': 87,\n",
       " 'sich': 88,\n",
       " 'erheben.': 89,\n",
       " '(Das': 90,\n",
       " 'erhebt': 91,\n",
       " 'Schweigeminute.)': 92,\n",
       " 'Frau': 93,\n",
       " 'Präsidentin,': 94,\n",
       " 'zur': 95,\n",
       " 'Geschäftsordnung.': 96,\n",
       " 'sicher': 97,\n",
       " 'aus': 98,\n",
       " 'Presse': 99,\n",
       " 'Fernsehen': 100,\n",
       " 'wissen,': 101,\n",
       " 'gab': 102,\n",
       " 'es': 103,\n",
       " 'Sri': 104,\n",
       " 'Lanka': 105,\n",
       " 'mehrere': 106,\n",
       " 'Bombenexplosionen': 107,\n",
       " 'mit': 108,\n",
       " 'zahlreichen': 109,\n",
       " 'Toten.': 110,\n",
       " 'Zu': 111,\n",
       " 'Attentatsopfern,': 112,\n",
       " 'jüngster': 113,\n",
       " 'Zeit': 114,\n",
       " 'beklagen': 115,\n",
       " 'gab,': 116,\n",
       " 'zählt': 117,\n",
       " 'Herr': 118,\n",
       " 'Kumar': 119,\n",
       " 'Ponnambalam,': 120,\n",
       " 'erst': 121,\n",
       " 'vor': 122,\n",
       " 'wenigen': 123,\n",
       " 'Monaten': 124,\n",
       " 'einen': 125,\n",
       " 'Besuch': 126,\n",
       " 'abgestattet': 127,\n",
       " 'hatte.': 128,\n",
       " 'Wäre': 129,\n",
       " 'angemessen,': 130,\n",
       " 'wenn': 131,\n",
       " 'Präsidentin': 132,\n",
       " 'einem': 133,\n",
       " 'Schreiben': 134,\n",
       " 'Bedauern': 135,\n",
       " 'gewaltsamen': 136,\n",
       " 'Tod': 137,\n",
       " 'Herrn': 138,\n",
       " 'Ponnambalam': 139,\n",
       " 'anderen': 140,\n",
       " 'Bürgern': 141,\n",
       " 'übermitteln': 142,\n",
       " 'sie': 143,\n",
       " 'auffordern': 144,\n",
       " 'würden,': 145,\n",
       " 'ihrem': 146,\n",
       " 'Kräften': 147,\n",
       " 'stehende': 148,\n",
       " 'tun,': 149,\n",
       " 'um': 150,\n",
       " 'friedlichen': 151,\n",
       " 'Lösung': 152,\n",
       " 'sehr': 153,\n",
       " 'schwierigen': 154,\n",
       " 'Situation': 155,\n",
       " 'suchen?': 156,\n",
       " 'Ja,': 157,\n",
       " 'Evans,': 158,\n",
       " 'denke,': 159,\n",
       " 'eine': 160,\n",
       " 'derartige': 161,\n",
       " 'Initiative': 162,\n",
       " 'durchaus': 163,\n",
       " 'angebracht': 164,\n",
       " 'ist.': 165,\n",
       " 'Wenn': 166,\n",
       " 'Haus': 167,\n",
       " 'damit': 168,\n",
       " 'einverstanden': 169,\n",
       " 'ist,': 170,\n",
       " 'werde': 171,\n",
       " 'Vorschlag': 172,\n",
       " 'Evans': 173,\n",
       " 'folgen.': 174,\n",
       " 'Könnten': 175,\n",
       " 'mir': 176,\n",
       " 'Auskunft': 177,\n",
       " 'Artikel': 178,\n",
       " '143': 179,\n",
       " 'Zusammenhang': 180,\n",
       " 'Unzulässigkeit': 181,\n",
       " 'geben?': 182,\n",
       " 'Meine': 183,\n",
       " 'Frage': 184,\n",
       " 'betrifft': 185,\n",
       " 'Angelegenheit,': 186,\n",
       " 'Donnerstag': 187,\n",
       " 'Sprache': 188,\n",
       " 'kommen': 189,\n",
       " 'wird': 190,\n",
       " 'auf': 191,\n",
       " 'dann': 192,\n",
       " 'erneut': 193,\n",
       " 'verweisen': 194,\n",
       " 'werde.': 195,\n",
       " 'Das': 196,\n",
       " 'Cunha-Bericht': 197,\n",
       " 'über': 198,\n",
       " 'mehrjährige': 199,\n",
       " 'Ausrichtungsprogramme': 200,\n",
       " 'befassen,': 201,\n",
       " 'Absatz': 202,\n",
       " '6': 203,\n",
       " 'vorschlägt,': 204,\n",
       " 'Länder,': 205,\n",
       " 'ihr': 206,\n",
       " 'Soll': 207,\n",
       " 'Flottenverkleinerung': 208,\n",
       " 'erfüllen,': 209,\n",
       " 'jährlich': 210,\n",
       " 'Art': 211,\n",
       " 'Quotenstrafe': 212,\n",
       " 'belegt': 213,\n",
       " 'werden': 214,\n",
       " 'sollen.': 215,\n",
       " 'Und': 216,\n",
       " 'zwar': 217,\n",
       " 'sollen': 218,\n",
       " 'Strafen': 219,\n",
       " 'trotz': 220,\n",
       " 'Grundsatzes': 221,\n",
       " 'relativen': 222,\n",
       " 'Stabilität': 223,\n",
       " 'verhängt': 224,\n",
       " 'werden.': 225,\n",
       " 'meine,': 226,\n",
       " 'Grundsatz': 227,\n",
       " 'elementaren': 228,\n",
       " 'Rechtsgrundsatz': 229,\n",
       " 'gemeinsamen': 230,\n",
       " 'Fischereipolitik': 231,\n",
       " 'darstellt': 232,\n",
       " 'ein': 233,\n",
       " 'Vorschlag,': 234,\n",
       " 'diesen': 235,\n",
       " 'unterlaufen,': 236,\n",
       " 'rechtlich': 237,\n",
       " 'unzulässig': 238,\n",
       " 'wäre.': 239,\n",
       " 'ob': 240,\n",
       " 'möglich': 241,\n",
       " 'Einwand': 242,\n",
       " 'gegen': 243,\n",
       " 'Dokument': 244,\n",
       " 'erheben,': 245,\n",
       " 'bei': 246,\n",
       " 'lediglich': 247,\n",
       " 'Bericht': 248,\n",
       " 'keinen': 249,\n",
       " 'Legislativvorschlag': 250,\n",
       " 'handelt,': 251,\n",
       " 'befugt': 252,\n",
       " 'bin,': 253,\n",
       " 'dies': 254,\n",
       " 'tun.': 255,\n",
       " 'Genau': 256,\n",
       " 'können': 257,\n",
       " 'wollen,': 258,\n",
       " 'diese': 259,\n",
       " 'ansprechen,': 260,\n",
       " 'd.': 261,\n",
       " 'h.': 262,\n",
       " 'Beginn': 263,\n",
       " 'Bericht.': 264,\n",
       " 'Präsidentin!': 265,\n",
       " 'Die': 266,\n",
       " 'erste': 267,\n",
       " 'diesjährige': 268,\n",
       " 'Tagung': 269,\n",
       " 'fällt': 270,\n",
       " 'leider': 271,\n",
       " 'zusammen,': 272,\n",
       " 'Vereinigten': 273,\n",
       " 'Staaten,': 274,\n",
       " 'Texas,': 275,\n",
       " 'Woche': 276,\n",
       " 'Hinrichtung': 277,\n",
       " 'eines': 278,\n",
       " 'Tode': 279,\n",
       " 'verurteilten': 280,\n",
       " '34jährigen': 281,\n",
       " 'jungen': 282,\n",
       " 'Mannes': 283,\n",
       " 'namens': 284,\n",
       " 'Hicks': 285,\n",
       " 'festgelegt': 286,\n",
       " 'worden': 287,\n",
       " 'Auf': 288,\n",
       " 'französischen': 289,\n",
       " 'Mitglieds,': 290,\n",
       " 'Zimeray,': 291,\n",
       " 'wurde': 292,\n",
       " 'bereits': 293,\n",
       " 'Petition': 294,\n",
       " 'eingereicht,': 295,\n",
       " 'vielen,': 296,\n",
       " 'selbst,': 297,\n",
       " 'unterzeichnet': 298,\n",
       " 'Gemäß': 299,\n",
       " 'vom': 300,\n",
       " 'gesamten': 301,\n",
       " 'Union': 302,\n",
       " 'nunmehr': 303,\n",
       " 'ständig': 304,\n",
       " 'vertretenen': 305,\n",
       " 'Linie': 306,\n",
       " 'jedoch': 307,\n",
       " 'bitten,': 308,\n",
       " 'ganzen': 309,\n",
       " 'Einfluß': 310,\n",
       " 'Ihres': 311,\n",
       " 'Amtes': 312,\n",
       " 'Institution,': 313,\n",
       " 'vertreten,': 314,\n",
       " 'Präsidentschaftskandidaten': 315,\n",
       " 'Gouverneur': 316,\n",
       " 'George': 317,\n",
       " 'W.': 318,\n",
       " 'Bush,': 319,\n",
       " 'Aussetzung': 320,\n",
       " 'Vollstreckung': 321,\n",
       " 'Todesurteils': 322,\n",
       " 'Begnadigung': 323,\n",
       " 'Verurteilten': 324,\n",
       " 'geltend': 325,\n",
       " 'machen.': 326,\n",
       " 'All': 327,\n",
       " 'entspricht': 328,\n",
       " 'Grundsätzen,': 329,\n",
       " 'wir': 330,\n",
       " 'stets': 331,\n",
       " 'verteidigt': 332,\n",
       " 'haben.': 333,\n",
       " 'Vielen': 334,\n",
       " 'Dank,': 335,\n",
       " 'Segni,': 336,\n",
       " 'will': 337,\n",
       " 'gerne': 338,\n",
       " 'ganz': 339,\n",
       " 'Sinne': 340,\n",
       " 'Position,': 341,\n",
       " 'als': 342,\n",
       " 'immer': 343,\n",
       " 'vertreten': 344,\n",
       " 'Fall': 345,\n",
       " 'aufmerksam': 346,\n",
       " 'machen,': 347,\n",
       " 'dieses': 348,\n",
       " 'wieder': 349,\n",
       " 'befaßt': 350,\n",
       " 'hat.': 351,\n",
       " 'Alexander': 352,\n",
       " 'Nikitin.': 353,\n",
       " 'Wir': 354,\n",
       " 'freuen': 355,\n",
       " 'uns': 356,\n",
       " 'hier': 357,\n",
       " 'alle,': 358,\n",
       " 'Gericht': 359,\n",
       " 'ihn': 360,\n",
       " 'freigesprochen': 361,\n",
       " 'deutlich': 362,\n",
       " 'gemacht': 363,\n",
       " 'hat,': 364,\n",
       " 'Rußland': 365,\n",
       " 'Zugang': 366,\n",
       " 'Umweltinformationen': 367,\n",
       " 'konstitutionelles': 368,\n",
       " 'Recht': 369,\n",
       " 'Nun': 370,\n",
       " 'aber': 371,\n",
       " 'so,': 372,\n",
       " 'er': 373,\n",
       " 'angeklagt': 374,\n",
       " 'soll,': 375,\n",
       " 'weil': 376,\n",
       " 'Staatsanwalt': 377,\n",
       " 'Berufung': 378,\n",
       " 'geht.': 379,\n",
       " 'wissen': 380,\n",
       " 'haben': 381,\n",
       " 'wirklich': 382,\n",
       " 'vielen': 383,\n",
       " 'Entschließungen': 384,\n",
       " 'festgestellt': 385,\n",
       " 'gerade': 386,\n",
       " 'während': 387,\n",
       " 'letzten': 388,\n",
       " 'Plenartagung': 389,\n",
       " 'vergangenen': 390,\n",
       " 'Jahres-,': 391,\n",
       " 'nur': 392,\n",
       " 'juristischer': 393,\n",
       " 'falsch': 394,\n",
       " 'Nikitin': 395,\n",
       " 'Kriminalität': 396,\n",
       " 'Verrat': 397,\n",
       " 'vorzuwerfen,': 398,\n",
       " 'Betroffene': 399,\n",
       " 'seinen': 400,\n",
       " 'Ergebnissen': 401,\n",
       " 'Nutzen': 402,\n",
       " 'Diese': 403,\n",
       " 'Ergebnisse': 404,\n",
       " 'Grundlage': 405,\n",
       " 'europäischen': 406,\n",
       " 'Programme': 407,\n",
       " 'Schutz': 408,\n",
       " 'Barentsee,': 409,\n",
       " 'deswegen': 410,\n",
       " 'Briefentwurf,': 411,\n",
       " 'wichtigsten': 412,\n",
       " 'Fakten': 413,\n",
       " 'schildert,': 414,\n",
       " 'prüfen': 415,\n",
       " 'Beschlüsse': 416,\n",
       " 'Position': 417,\n",
       " 'Schroedter,': 418,\n",
       " 'bin': 419,\n",
       " 'bereit,': 420,\n",
       " 'zusammenhängenden': 421,\n",
       " 'prüfen,': 422,\n",
       " 'Ihr': 423,\n",
       " 'Brief': 424,\n",
       " 'vorliegt.': 425,\n",
       " 'zunächst': 426,\n",
       " 'besten': 427,\n",
       " 'Dank': 428,\n",
       " 'dafür,': 429,\n",
       " 'Wort': 430,\n",
       " 'gehalten': 431,\n",
       " 'nun': 432,\n",
       " 'ersten': 433,\n",
       " 'neuen': 434,\n",
       " 'Jahres': 435,\n",
       " 'Angebot': 436,\n",
       " 'an': 437,\n",
       " 'Fernsehprogrammen': 438,\n",
       " 'unseren': 439,\n",
       " 'Büros': 440,\n",
       " 'tatsächlich': 441,\n",
       " 'enorm': 442,\n",
       " 'erweitert': 443,\n",
       " 'Dennoch,': 444,\n",
       " 'meinem': 445,\n",
       " 'entsprochen.': 446,\n",
       " 'Zwar': 447,\n",
       " 'jetzt': 448,\n",
       " 'zwei': 449,\n",
       " 'finnische': 450,\n",
       " 'portugiesischen,': 451,\n",
       " 'wie': 452,\n",
       " 'niederländischen': 453,\n",
       " 'Sender': 454,\n",
       " 'empfangen.': 455,\n",
       " 'hatte': 456,\n",
       " 'niederländisches': 457,\n",
       " 'Programm': 458,\n",
       " 'gebeten,': 459,\n",
       " 'denn': 460,\n",
       " 'Niederländer': 461,\n",
       " 'möchten': 462,\n",
       " 'Nachrichten': 463,\n",
       " 'verfolgen,': 464,\n",
       " 'jeden': 465,\n",
       " 'Monat': 466,\n",
       " 'hierher': 467,\n",
       " 'Verbannung': 468,\n",
       " 'geschickt': 469,\n",
       " 'Deshalb': 470,\n",
       " 'ersuchen,': 471,\n",
       " 'dafür': 472,\n",
       " 'Sorge': 473,\n",
       " 'tragen,': 474,\n",
       " 'niederländischer': 475,\n",
       " 'eingespeist': 476,\n",
       " 'wird.': 477,\n",
       " 'Plooij-van': 478,\n",
       " 'Gorsel,': 479,\n",
       " 'kann': 480,\n",
       " 'mitteilen,': 481,\n",
       " 'Punkt': 482,\n",
       " 'Mittwoch': 483,\n",
       " 'Tagesordnung': 484,\n",
       " 'Quästoren': 485,\n",
       " 'steht.': 486,\n",
       " 'dort': 487,\n",
       " 'Ihrem': 488,\n",
       " 'entschieden': 489,\n",
       " 'sagen,': 490,\n",
       " 'warum': 491,\n",
       " 'Arbeitsschutzregelungen': 492,\n",
       " 'hält,': 493,\n",
       " 'selbst': 494,\n",
       " 'verabschiedet': 495,\n",
       " 'hat?': 496,\n",
       " 'Weshalb': 497,\n",
       " 'Luftqualität': 498,\n",
       " 'diesem': 499,\n",
       " 'Gebäude': 500,\n",
       " 'seit': 501,\n",
       " 'Wahl': 502,\n",
       " 'einziges': 503,\n",
       " 'Mal': 504,\n",
       " 'überprüft?': 505,\n",
       " 'Arbeitsschutzausschuß': 506,\n",
       " '1998': 507,\n",
       " 'zusammengetreten?': 508,\n",
       " 'Warum': 509,\n",
       " 'hat': 510,\n",
       " 'weder': 511,\n",
       " 'Brüsseler': 512,\n",
       " 'noch': 513,\n",
       " 'Straßburger': 514,\n",
       " 'Parlamentsgebäude': 515,\n",
       " 'Brandschutzübung': 516,\n",
       " 'stattgefunden?': 517,\n",
       " 'finden': 518,\n",
       " 'keine': 519,\n",
       " 'Brandschutzbelehrungen': 520,\n",
       " 'statt?': 521,\n",
       " 'Unfall': 522,\n",
       " 'nichts': 523,\n",
       " 'unternommen,': 524,\n",
       " 'Treppen': 525,\n",
       " 'sicherer': 526,\n",
       " 'machen?': 527,\n",
       " 'Nichtraucherzonen': 528,\n",
       " 'Rauchverbot': 529,\n",
       " 'durchgesetzt?': 530,\n",
       " 'Es': 531,\n",
       " 'Schande,': 532,\n",
       " 'Regeln': 533,\n",
       " 'verabschieden,': 534,\n",
       " 'halten.': 535,\n",
       " 'Lynne,': 536,\n",
       " 'völlig': 537,\n",
       " 'recht,': 538,\n",
       " 'all': 539,\n",
       " 'so': 540,\n",
       " 'unterbreiten.': 541,\n",
       " 'sicher,': 542,\n",
       " 'großen': 543,\n",
       " 'Wert': 544,\n",
       " 'darauf': 545,\n",
       " 'legen,': 546,\n",
       " 'Rechtsvorschriften,': 547,\n",
       " 'einhalten.': 548,\n",
       " 'Díez': 549,\n",
       " 'González': 550,\n",
       " 'hatten': 551,\n",
       " 'einige': 552,\n",
       " 'Anfragen': 553,\n",
       " 'bestimmten,': 554,\n",
       " 'spanischen': 555,\n",
       " 'Zeitung': 556,\n",
       " 'wiedergegebenen': 557,\n",
       " 'Stellungnahmen': 558,\n",
       " 'Vizepräsidentin,': 559,\n",
       " 'de': 560,\n",
       " 'Palacio,': 561,\n",
       " 'gestellt.': 562,\n",
       " 'zuständigen': 563,\n",
       " 'Dienste': 564,\n",
       " 'aufgenommen,': 565,\n",
       " 'da': 566,\n",
       " 'Meinung': 567,\n",
       " 'waren,': 568,\n",
       " 'seien': 569,\n",
       " 'schon': 570,\n",
       " 'vorangegangenen': 571,\n",
       " 'Sitzung': 572,\n",
       " 'beantwortet': 573,\n",
       " 'worden.': 574,\n",
       " 'bitte,': 575,\n",
       " 'Entscheidung': 576,\n",
       " 'überdenken,': 577,\n",
       " 'früher': 578,\n",
       " 'beantworteten': 579,\n",
       " 'bezogen': 580,\n",
       " 'Auftreten': 581,\n",
       " 'Palacio': 582,\n",
       " 'bestimmten': 583,\n",
       " '18.': 584,\n",
       " 'November': 585,\n",
       " 'Tageszeitung': 586,\n",
       " 'ABC': 587,\n",
       " 'erschienenen': 588,\n",
       " 'Erklärungen.': 589,\n",
       " 'Lieber': 590,\n",
       " 'Kollege,': 591,\n",
       " 'prüfen.': 592,\n",
       " 'muß': 593,\n",
       " 'Lage': 594,\n",
       " 'Moment': 595,\n",
       " 'etwas': 596,\n",
       " 'verworren': 597,\n",
       " 'halte.': 598,\n",
       " 'genau': 599,\n",
       " 'seine': 600,\n",
       " 'Richtigkeit': 601,\n",
       " 'wüßte': 602,\n",
       " 'gern,': 603,\n",
       " 'deutliches': 604,\n",
       " 'Signal': 605,\n",
       " 'Unzufriedenheit': 606,\n",
       " 'bezüglich': 607,\n",
       " 'heutigen': 608,\n",
       " 'Entscheidung,': 609,\n",
       " 'Verlängerung': 610,\n",
       " 'Waffenembargos': 611,\n",
       " 'Indonesien': 612,\n",
       " 'abgelehnt': 613,\n",
       " 'wird,': 614,\n",
       " 'aussenden': 615,\n",
       " 'zumal': 616,\n",
       " 'große': 617,\n",
       " 'Mehrheit': 618,\n",
       " 'Vergangenheit': 619,\n",
       " 'Waffenembargo': 620,\n",
       " 'ausgesprochen': 621,\n",
       " 'heutige': 622,\n",
       " 'Embargos': 623,\n",
       " 'birgt': 624,\n",
       " 'angesichts': 625,\n",
       " 'dortigen': 626,\n",
       " 'Gefahr.': 627,\n",
       " 'sollte,': 628,\n",
       " 'entspricht,': 629,\n",
       " 'entsprechende': 630,\n",
       " 'Botschaft': 631,\n",
       " 'senden.': 632,\n",
       " 'Ablehnung': 633,\n",
       " 'seitens': 634,\n",
       " 'EU-Mitgliedstaaten': 635,\n",
       " 'unverantwortlich.': 636,\n",
       " 'wurde,': 637,\n",
       " 'äußerst': 638,\n",
       " 'instabil.': 639,\n",
       " 'sogar': 640,\n",
       " 'Gefahr': 641,\n",
       " 'Militärputsches.': 642,\n",
       " 'nicht,': 643,\n",
       " 'was': 644,\n",
       " 'passiert.': 645,\n",
       " 'also': 646,\n",
       " 'sollten': 647,\n",
       " 'Waffenhersteller': 648,\n",
       " 'EU': 649,\n",
       " 'Kosten': 650,\n",
       " 'unschuldiger': 651,\n",
       " 'Menschen': 652,\n",
       " 'Profite': 653,\n",
       " 'einstreichen?': 654,\n",
       " 'Dieser': 655,\n",
       " 'bisher': 656,\n",
       " 'Dringlichkeitsdebatte': 657,\n",
       " 'vorgesehen.': 658,\n",
       " 'Arbeitsplan': 659,\n",
       " 'Nach': 660,\n",
       " 'folgt': 661,\n",
       " 'Prüfung': 662,\n",
       " 'endgültigen': 663,\n",
       " 'Entwurfs': 664,\n",
       " 'Tagesordnung,': 665,\n",
       " '110': 666,\n",
       " 'Geschäftsordnung': 667,\n",
       " 'Donnerstag,': 668,\n",
       " '13.': 669,\n",
       " 'Januar': 670,\n",
       " 'Konferenz': 671,\n",
       " 'Präsidenten': 672,\n",
       " 'wurde.': 673,\n",
       " 'Montag': 674,\n",
       " 'Dienstag': 675,\n",
       " 'liegen': 676,\n",
       " 'Änderungen': 677,\n",
       " 'vor.': 678,\n",
       " 'Zum': 679,\n",
       " 'Mittwoch:': 680,\n",
       " 'Sozialdemokratische': 681,\n",
       " 'Fraktion': 682,\n",
       " 'beantragt,': 683,\n",
       " 'Erklärung': 684,\n",
       " 'Kommission': 685,\n",
       " 'ihre': 686,\n",
       " 'strategischen': 687,\n",
       " 'Ziele': 688,\n",
       " 'fünf': 689,\n",
       " 'Jahre': 690,\n",
       " 'sowie': 691,\n",
       " 'Verwaltungsreform': 692,\n",
       " 'aufzunehmen.': 693,\n",
       " 'Antragsteller,': 694,\n",
       " 'Barón': 695,\n",
       " 'Crespo,': 696,\n",
       " 'Antrag': 697,\n",
       " 'begründen,': 698,\n",
       " 'falls': 699,\n",
       " 'wünscht.': 700,\n",
       " 'Danach': 701,\n",
       " 'verfahren': 702,\n",
       " 'üblich:': 703,\n",
       " 'Redner': 704,\n",
       " 'dagegen.': 705,\n",
       " 'Vorstellung': 706,\n",
       " 'politischen': 707,\n",
       " 'Programms': 708,\n",
       " 'Prodi': 709,\n",
       " 'gesamte': 710,\n",
       " 'Wahlperiode': 711,\n",
       " 'ging': 712,\n",
       " 'Sozialdemokratischen': 713,\n",
       " 'Partei': 714,\n",
       " 'Europas': 715,\n",
       " 'zurück,': 716,\n",
       " 'einhellige': 717,\n",
       " 'Billigung': 718,\n",
       " 'September': 719,\n",
       " 'ausdrückliche': 720,\n",
       " 'Zustimmung': 721,\n",
       " 'Präsident': 722,\n",
       " 'fand,': 723,\n",
       " 'Zusage': 724,\n",
       " 'seiner': 725,\n",
       " 'Antrittsrede': 726,\n",
       " 'bekräftigte.': 727,\n",
       " 'insofern': 728,\n",
       " 'Bedeutung,': 729,\n",
       " 'Organ': 730,\n",
       " 'Verträgen': 731,\n",
       " 'Initiativmonopol': 732,\n",
       " 'besitzt': 733,\n",
       " 'somit': 734,\n",
       " 'grundlegend': 735,\n",
       " 'politische': 736,\n",
       " 'legislative': 737,\n",
       " 'Tätigkeit': 738,\n",
       " 'Jahren': 739,\n",
       " 'gestaltet.': 740,\n",
       " 'daran': 741,\n",
       " 'erinnern,': 742,\n",
       " 'zweimal': 743,\n",
       " 'sein': 744,\n",
       " 'Vertrauen': 745,\n",
       " 'hat;': 746,\n",
       " 'sprach': 747,\n",
       " 'ihm': 748,\n",
       " 'Juli': 749,\n",
       " 'aus,': 750,\n",
       " 'dann,': 751,\n",
       " 'neue': 752,\n",
       " 'Amt': 753,\n",
       " 'war,': 754,\n",
       " 'Vertrauensvotum': 755,\n",
       " 'insgesamt.': 756,\n",
       " 'Somit': 757,\n",
       " 'genügend': 758,\n",
       " 'Zeit,': 759,\n",
       " 'erarbeiten,': 760,\n",
       " 'wir,': 761,\n",
       " 'kennenlernen': 762,\n",
       " 'erklären': 763,\n",
       " 'können.': 764,\n",
       " 'In': 765,\n",
       " 'erinnere': 766,\n",
       " 'Entschließung': 767,\n",
       " '15.': 768,\n",
       " 'September,': 769,\n",
       " 'empfohlen': 770,\n",
       " 'kürzestmöglichen': 771,\n",
       " 'Frist': 772,\n",
       " 'vorzulegen.': 773,\n",
       " 'Ereignisse': 774,\n",
       " 'Rande': 775,\n",
       " 'ihren': 776,\n",
       " 'Anfang': 777,\n",
       " 'nahmen': 778,\n",
       " 'wobei': 779,\n",
       " 'Bestätigung': 780,\n",
       " 'Ratifizierung': 781,\n",
       " 'außerhalb': 782,\n",
       " 'gefaßten': 783,\n",
       " 'Beschlüssen': 784,\n",
       " 'genutzt': 785,\n",
       " 'verdeutlichen': 786,\n",
       " 'Dilemma:': 787,\n",
       " 'Entweder': 788,\n",
       " 'Lage,': 789,\n",
       " 'vorzulegen': 790,\n",
       " '(In': 791,\n",
       " 'sollte': 792,\n",
       " 'Klärung': 793,\n",
       " 'herbeiführen.': 794,\n",
       " 'Worten': 795,\n",
       " 'ihres': 796,\n",
       " 'dazu': 797,\n",
       " 'Lage.': 798,\n",
       " 'Da': 799,\n",
       " 'durch': 800,\n",
       " 'halte': 801,\n",
       " 'zweckmäßig,': 802,\n",
       " 'Abstimmung': 803,\n",
       " 'hinsichtlich': 804,\n",
       " 'ihrer': 805,\n",
       " 'Bereitschaft': 806,\n",
       " 'Programms,': 807,\n",
       " 'vereinbart': 808,\n",
       " 'erfahren.),': 809,\n",
       " 'oder': 810,\n",
       " 'offenbar': 811,\n",
       " 'vorgeben.': 812,\n",
       " 'meiner': 813,\n",
       " 'Ansicht': 814,\n",
       " 'würde': 815,\n",
       " 'zweite': 816,\n",
       " 'Hypothese': 817,\n",
       " 'Verzicht': 818,\n",
       " 'unsere': 819,\n",
       " 'Verantwortung': 820,\n",
       " 'darüber': 821,\n",
       " 'hinaus': 822,\n",
       " 'Aufwerfen': 823,\n",
       " 'originellen': 824,\n",
       " 'These,': 825,\n",
       " 'unbekannten': 826,\n",
       " 'Methode': 827,\n",
       " 'gleichkommen,': 828,\n",
       " 'darin': 829,\n",
       " 'bestände,': 830,\n",
       " 'Fraktionen': 831,\n",
       " 'programmatische': 832,\n",
       " 'Rede': 833,\n",
       " 'schriftlicher': 834,\n",
       " 'Form': 835,\n",
       " 'vorher': 836,\n",
       " 'vereinbart,': 837,\n",
       " 'Tag': 838,\n",
       " 'zuvor': 839,\n",
       " 'Kenntnis': 840,\n",
       " 'geben,': 841,\n",
       " 'berücksichtigen': 842,\n",
       " 'Legislativprogramm': 843,\n",
       " 'Februar': 844,\n",
       " 'diskutiert': 845,\n",
       " 'verzichten': 846,\n",
       " 'könnten,': 847,\n",
       " 'Internet': 848,\n",
       " 'alle': 849,\n",
       " 'informiert': 850,\n",
       " 'würden': 851,\n",
       " 'Grund': 852,\n",
       " 'mehr': 853,\n",
       " 'hätte,': 854,\n",
       " 'Angelegenheit': 855,\n",
       " 'befassen.': 856,\n",
       " 'meine': 857,\n",
       " 'zuzuhören,': 858,\n",
       " 'diskutieren': 859,\n",
       " 'nachzudenken,': 860,\n",
       " 'gibt': 861,\n",
       " 'Rechtfertigung': 862,\n",
       " 'Verzögerung,': 863,\n",
       " 'glauben,': 864,\n",
       " 'ursprüngliche': 865,\n",
       " 'Vereinbarung': 866,\n",
       " 'zwischen': 867,\n",
       " 'Kraft': 868,\n",
       " 'setzen': 869,\n",
       " 'verantwortungsbewußt': 870,\n",
       " 'Mitbürgerinnen': 871,\n",
       " 'Mitbürgern': 872,\n",
       " 'treten': 873,\n",
       " 'Europas,': 874,\n",
       " 'erwähnt': 875,\n",
       " 'haben,': 876,\n",
       " 'darin,': 877,\n",
       " 'Termin': 878,\n",
       " 'beizubehalten,': 879,\n",
       " 'Verwaltungsreformprojekt': 880,\n",
       " 'einzubeziehen,': 881,\n",
       " 'andernfalls': 882,\n",
       " 'paradoxe': 883,\n",
       " 'geraten': 884,\n",
       " 'könnten:': 885,\n",
       " 'Mit': 886,\n",
       " 'Ausrede,': 887,\n",
       " 'Wortlaut': 888,\n",
       " 'liege': 889,\n",
       " 'vor,': 890,\n",
       " 'einerseits': 891,\n",
       " 'abgesprochen,': 892,\n",
       " 'sprechen,': 893,\n",
       " 'andererseits': 894,\n",
       " 'Reform': 895,\n",
       " 'stattfinden,': 896,\n",
       " 'ohne': 897,\n",
       " 'Texte': 898,\n",
       " 'lesen': 899,\n",
       " 'konnte,': 900,\n",
       " 'zugrunde': 901,\n",
       " 'liegen.': 902,\n",
       " 'Daher': 903,\n",
       " 'äußern,': 904,\n",
       " 'danach': 905,\n",
       " 'schreiten.': 906,\n",
       " '(Beifall': 907,\n",
       " 'PSE-Fraktion)': 908,\n",
       " 'liebe': 909,\n",
       " 'Kollegen!': 910,\n",
       " 'doch': 911,\n",
       " 'erstaunt': 912,\n",
       " 'Verhalten': 913,\n",
       " 'verlangt,': 914,\n",
       " 'Tagesordnungspunkt': 915,\n",
       " 'gesetzt': 916,\n",
       " 'Kollege': 917,\n",
       " 'konnten': 918,\n",
       " 'anwesend': 919,\n",
       " 'sein.': 920,\n",
       " 'kritisiere': 921,\n",
       " 'nicht;': 922,\n",
       " 'kommt': 923,\n",
       " 'mal': 924,\n",
       " 'man': 925,\n",
       " 'läßt.': 926,\n",
       " 'Der': 927,\n",
       " 'Hänsch': 928,\n",
       " 'vertreten.': 929,\n",
       " 'ausführliche': 930,\n",
       " 'Debatte': 931,\n",
       " 'geführt.': 932,\n",
       " 'Nur': 933,\n",
       " 'Ihre': 934,\n",
       " 'sagen.': 935,\n",
       " 'abgestimmt.': 936,\n",
       " 'Jeder': 937,\n",
       " 'Vorsitzende': 938,\n",
       " 'bzw.': 939,\n",
       " 'jede': 940,\n",
       " 'ja': 941,\n",
       " 'viele': 942,\n",
       " 'Stimmen,': 943,\n",
       " 'Mitglieder': 944,\n",
       " 'Punkt.': 945,\n",
       " 'Erinnerung': 946,\n",
       " 'ausgegangen:': 947,\n",
       " '422': 948,\n",
       " '180': 949,\n",
       " 'Stimmen': 950,\n",
       " 'einigen': 951,\n",
       " 'Enthaltungen.': 952,\n",
       " 'heißt,': 953,\n",
       " 'Fraktionen,': 954,\n",
       " 'Ausnahme': 955,\n",
       " 'Fraktionslosen': 956,\n",
       " 'waren': 957,\n",
       " 'einig,': 958,\n",
       " 'war': 959,\n",
       " 'Meinung,': 960,\n",
       " 'verfahren,': 961,\n",
       " 'vorgeschlagen': 962,\n",
       " 'Alle': 963,\n",
       " 'anderer': 964,\n",
       " 'Meinung.': 965,\n",
       " 'Beschluß.': 966,\n",
       " 'Jetzt': 967,\n",
       " 'Sache': 968,\n",
       " 'Kommission,': 969,\n",
       " 'Romano': 970,\n",
       " 'Prodi,': 971,\n",
       " 'Prozeß,': 972,\n",
       " 'jeder': 973,\n",
       " 'weiß,': 974,\n",
       " 'ausgesprochen.': 975,\n",
       " 'Aber': 976,\n",
       " 'Strategie': 977,\n",
       " 'geordneten': 978,\n",
       " 'Verfahren': 979,\n",
       " 'führen': 980,\n",
       " 'müssen,': 981,\n",
       " 'aufgrund': 982,\n",
       " 'mündlichen': 983,\n",
       " 'Parlament,': 984,\n",
       " 'sondern': 985,\n",
       " 'Dokumentes,': 986,\n",
       " 'beschlossen': 987,\n",
       " 'beschreibt.': 988,\n",
       " 'Ein': 989,\n",
       " 'solches': 990,\n",
       " 'nicht!': 991,\n",
       " 'Jahr': 992,\n",
       " '2000': 993,\n",
       " 'vorlegen.': 994,\n",
       " 'gesagt,': 995,\n",
       " 'o.': 996,\n",
       " 'k,': 997,\n",
       " 'machen': 998,\n",
       " 'will,': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S开始     E结束       P空字符标记\n",
    "# 训练集，是使用两个文件表示翻译前后的内容\n",
    "src_file = '/home/shenc/Desktop/wmt14/europarl-v7.de-en.de'\n",
    "tgt_file = '/home/shenc/Desktop/wmt14/europarl-v7.de-en.en'\n",
    "\n",
    "# 从文件中读数据\n",
    "with open(src_file, 'r', encoding='utf-8') as f_src, open(tgt_file, 'r', encoding='utf-8') as f_tgt:\n",
    "    src_sentences = f_src.readlines()\n",
    "    tgt_sentences = f_tgt.readlines()\n",
    "\n",
    "# 从这里获得最大序列长度，方便以后\n",
    "src_vocab, src_idx2word, max_len_src = build_vocab(src_sentences)\n",
    "tgt_vocab, tgt_idx2word, max_len_tgt = build_vocab(tgt_sentences)\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(\n",
    "    src_file, tgt_file, src_vocab, tgt_vocab, max_len_src, max_len_tgt\n",
    ")\n",
    "\n",
    "src_len = max_len_src\n",
    "tgt_len = max_len_tgt\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "d_model = 512  # Embedding Size（token embedding和position编码的维度）\n",
    "\n",
    "d_ff = 2048\n",
    "d_k = d_v = 64  # dimension of K(=Q), V（Q和K的维度需要相同，这里为了方便让K=V）\n",
    "n_layers = 6  # number of Encoder of Decoder Layer（Block的个数）\n",
    "n_heads = 8  # 有几个头hhh\n",
    "\n",
    "\n",
    "loader = Data.DataLoader(\n",
    "    MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss = 2.793939\n",
      "Epoch: 0001 loss = 2.622025\n",
      "Epoch: 0002 loss = 2.453482\n",
      "Epoch: 0002 loss = 2.383991\n",
      "Epoch: 0003 loss = 2.218930\n",
      "Epoch: 0003 loss = 1.631452\n",
      "Epoch: 0004 loss = 1.645802\n",
      "Epoch: 0004 loss = 1.601075\n",
      "Epoch: 0005 loss = 1.179267\n",
      "Epoch: 0005 loss = 1.340614\n",
      "Epoch: 0006 loss = 0.965044\n",
      "Epoch: 0006 loss = 0.678392\n",
      "Epoch: 0007 loss = 0.552932\n",
      "Epoch: 0007 loss = 0.631039\n",
      "Epoch: 0008 loss = 0.398819\n",
      "Epoch: 0008 loss = 0.395581\n",
      "Epoch: 0009 loss = 0.269114\n",
      "Epoch: 0009 loss = 0.259883\n",
      "Epoch: 0010 loss = 0.202923\n",
      "Epoch: 0010 loss = 0.211473\n",
      "Epoch: 0011 loss = 0.218503\n",
      "Epoch: 0011 loss = 0.202608\n",
      "Epoch: 0012 loss = 0.121742\n",
      "Epoch: 0012 loss = 0.268149\n",
      "Epoch: 0013 loss = 0.164403\n",
      "Epoch: 0013 loss = 0.083710\n",
      "Epoch: 0014 loss = 0.153977\n",
      "Epoch: 0014 loss = 0.056936\n",
      "Epoch: 0015 loss = 0.137254\n",
      "Epoch: 0015 loss = 0.079034\n",
      "Epoch: 0016 loss = 0.066254\n",
      "Epoch: 0016 loss = 0.152756\n",
      "Epoch: 0017 loss = 0.090989\n",
      "Epoch: 0017 loss = 0.131057\n",
      "Epoch: 0018 loss = 0.127934\n",
      "Epoch: 0018 loss = 0.035977\n",
      "Epoch: 0019 loss = 0.091409\n",
      "Epoch: 0019 loss = 0.036883\n",
      "Epoch: 0020 loss = 0.042585\n",
      "Epoch: 0020 loss = 0.173486\n",
      "Epoch: 0021 loss = 0.126337\n",
      "Epoch: 0021 loss = 0.006874\n",
      "Epoch: 0022 loss = 0.090541\n",
      "Epoch: 0022 loss = 0.003973\n",
      "Epoch: 0023 loss = 0.072767\n",
      "Epoch: 0023 loss = 0.004133\n",
      "Epoch: 0024 loss = 0.081441\n",
      "Epoch: 0024 loss = 0.005634\n",
      "Epoch: 0025 loss = 0.093819\n",
      "Epoch: 0025 loss = 0.008918\n",
      "Epoch: 0026 loss = 0.059263\n",
      "Epoch: 0026 loss = 0.007861\n",
      "Epoch: 0027 loss = 0.056269\n",
      "Epoch: 0027 loss = 0.014232\n",
      "Epoch: 0028 loss = 0.022055\n",
      "Epoch: 0028 loss = 0.015927\n",
      "Epoch: 0029 loss = 0.031814\n",
      "Epoch: 0029 loss = 0.017737\n",
      "Epoch: 0030 loss = 0.032781\n",
      "Epoch: 0030 loss = 0.008339\n",
      "Epoch: 0031 loss = 0.036307\n",
      "Epoch: 0031 loss = 0.032848\n",
      "Epoch: 0032 loss = 0.075816\n",
      "Epoch: 0032 loss = 0.003031\n",
      "Epoch: 0033 loss = 0.023080\n",
      "Epoch: 0033 loss = 0.004182\n",
      "Epoch: 0034 loss = 0.003370\n",
      "Epoch: 0034 loss = 0.038487\n",
      "Epoch: 0035 loss = 0.009703\n",
      "Epoch: 0035 loss = 0.006741\n",
      "Epoch: 0036 loss = 0.013772\n",
      "Epoch: 0036 loss = 0.001226\n",
      "Epoch: 0037 loss = 0.005099\n",
      "Epoch: 0037 loss = 0.016196\n",
      "Epoch: 0038 loss = 0.001946\n",
      "Epoch: 0038 loss = 0.029930\n",
      "Epoch: 0039 loss = 0.009035\n",
      "Epoch: 0039 loss = 0.006830\n",
      "Epoch: 0040 loss = 0.010425\n",
      "Epoch: 0040 loss = 0.006521\n",
      "Epoch: 0041 loss = 0.015334\n",
      "Epoch: 0041 loss = 0.000342\n",
      "Epoch: 0042 loss = 0.002774\n",
      "Epoch: 0042 loss = 0.040820\n",
      "Epoch: 0043 loss = 0.002344\n",
      "Epoch: 0043 loss = 0.021322\n",
      "Epoch: 0044 loss = 0.008174\n",
      "Epoch: 0044 loss = 0.002632\n",
      "Epoch: 0045 loss = 0.002214\n",
      "Epoch: 0045 loss = 0.008764\n",
      "Epoch: 0046 loss = 0.007316\n",
      "Epoch: 0046 loss = 0.001081\n",
      "Epoch: 0047 loss = 0.002949\n",
      "Epoch: 0047 loss = 0.023474\n",
      "Epoch: 0048 loss = 0.013503\n",
      "Epoch: 0048 loss = 0.004833\n",
      "Epoch: 0049 loss = 0.002796\n",
      "Epoch: 0049 loss = 0.038896\n",
      "Epoch: 0050 loss = 0.017770\n",
      "Epoch: 0050 loss = 0.004219\n",
      "Epoch: 0051 loss = 0.001070\n",
      "Epoch: 0051 loss = 0.010907\n",
      "Epoch: 0052 loss = 0.003045\n",
      "Epoch: 0052 loss = 0.001848\n",
      "Epoch: 0053 loss = 0.001005\n",
      "Epoch: 0053 loss = 0.001407\n",
      "Epoch: 0054 loss = 0.001030\n",
      "Epoch: 0054 loss = 0.000154\n",
      "Epoch: 0055 loss = 0.000113\n",
      "Epoch: 0055 loss = 0.001127\n",
      "Epoch: 0056 loss = 0.001229\n",
      "Epoch: 0056 loss = 0.000142\n",
      "Epoch: 0057 loss = 0.001479\n",
      "Epoch: 0057 loss = 0.000052\n",
      "Epoch: 0058 loss = 0.001254\n",
      "Epoch: 0058 loss = 0.000793\n",
      "Epoch: 0059 loss = 0.001796\n",
      "Epoch: 0059 loss = 0.000311\n",
      "Epoch: 0060 loss = 0.004251\n",
      "Epoch: 0060 loss = 0.000027\n",
      "Epoch: 0061 loss = 0.000267\n",
      "Epoch: 0061 loss = 0.007454\n",
      "Epoch: 0062 loss = 0.008633\n",
      "Epoch: 0062 loss = 0.000035\n",
      "Epoch: 0063 loss = 0.010786\n",
      "Epoch: 0063 loss = 0.000050\n",
      "Epoch: 0064 loss = 0.005178\n",
      "Epoch: 0064 loss = 0.025876\n",
      "Epoch: 0065 loss = 0.004204\n",
      "Epoch: 0065 loss = 0.002650\n",
      "Epoch: 0066 loss = 0.004553\n",
      "Epoch: 0066 loss = 0.009635\n",
      "Epoch: 0067 loss = 0.003501\n",
      "Epoch: 0067 loss = 0.001240\n",
      "Epoch: 0068 loss = 0.001260\n",
      "Epoch: 0068 loss = 0.000069\n",
      "Epoch: 0069 loss = 0.005434\n",
      "Epoch: 0069 loss = 0.000111\n",
      "Epoch: 0070 loss = 0.000372\n",
      "Epoch: 0070 loss = 0.000455\n",
      "Epoch: 0071 loss = 0.000404\n",
      "Epoch: 0071 loss = 0.000212\n",
      "Epoch: 0072 loss = 0.000285\n",
      "Epoch: 0072 loss = 0.000532\n",
      "Epoch: 0073 loss = 0.000921\n",
      "Epoch: 0073 loss = 0.000081\n",
      "Epoch: 0074 loss = 0.000358\n",
      "Epoch: 0074 loss = 0.000076\n",
      "Epoch: 0075 loss = 0.000503\n",
      "Epoch: 0075 loss = 0.000125\n",
      "Epoch: 0076 loss = 0.000865\n",
      "Epoch: 0076 loss = 0.000092\n",
      "Epoch: 0077 loss = 0.001207\n",
      "Epoch: 0077 loss = 0.000088\n",
      "Epoch: 0078 loss = 0.002957\n",
      "Epoch: 0078 loss = 0.000122\n",
      "Epoch: 0079 loss = 0.000389\n",
      "Epoch: 0079 loss = 0.000080\n",
      "Epoch: 0080 loss = 0.000080\n",
      "Epoch: 0080 loss = 0.003791\n",
      "Epoch: 0081 loss = 0.001805\n",
      "Epoch: 0081 loss = 0.000071\n",
      "Epoch: 0082 loss = 0.001217\n",
      "Epoch: 0082 loss = 0.000066\n",
      "Epoch: 0083 loss = 0.000430\n",
      "Epoch: 0083 loss = 0.000082\n",
      "Epoch: 0084 loss = 0.000088\n",
      "Epoch: 0084 loss = 0.003329\n",
      "Epoch: 0085 loss = 0.000912\n",
      "Epoch: 0085 loss = 0.000091\n",
      "Epoch: 0086 loss = 0.000049\n",
      "Epoch: 0086 loss = 0.001378\n",
      "Epoch: 0087 loss = 0.000058\n",
      "Epoch: 0087 loss = 0.002723\n",
      "Epoch: 0088 loss = 0.001145\n",
      "Epoch: 0088 loss = 0.000138\n",
      "Epoch: 0089 loss = 0.001122\n",
      "Epoch: 0089 loss = 0.000031\n",
      "Epoch: 0090 loss = 0.000124\n",
      "Epoch: 0090 loss = 0.008691\n",
      "Epoch: 0091 loss = 0.001078\n",
      "Epoch: 0091 loss = 0.000023\n",
      "Epoch: 0092 loss = 0.000860\n",
      "Epoch: 0092 loss = 0.000066\n",
      "Epoch: 0093 loss = 0.000164\n",
      "Epoch: 0093 loss = 0.000689\n",
      "Epoch: 0094 loss = 0.000872\n",
      "Epoch: 0094 loss = 0.000085\n",
      "Epoch: 0095 loss = 0.000208\n",
      "Epoch: 0095 loss = 0.001279\n",
      "Epoch: 0096 loss = 0.001293\n",
      "Epoch: 0096 loss = 0.000552\n",
      "Epoch: 0097 loss = 0.000209\n",
      "Epoch: 0097 loss = 0.001423\n",
      "Epoch: 0098 loss = 0.000224\n",
      "Epoch: 0098 loss = 0.000452\n",
      "Epoch: 0099 loss = 0.000457\n",
      "Epoch: 0099 loss = 0.000462\n",
      "Epoch: 0100 loss = 0.000268\n",
      "Epoch: 0100 loss = 0.000557\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================================================================================================\n",
    "# Transformer模型\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        '''\n",
    "        初始化位置编码类\n",
    "        d_model:    模型的维度，即每个词的嵌入维度\n",
    "        dropout:    防止过拟合的dropout\n",
    "        max_len:    最大序列长度，用于生成位置编码\n",
    "        '''\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model) # 用于存储位置编码\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # 偶数维度使用正弦，奇数维使用余弦\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # 添加一个batch维度并转置\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        # 将位置编码存储为不可训练的缓冲区\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        将位置编码添加到张量x中\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        加入位置编码并dropout之后的张量\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    \"\"\"\n",
    "    创建 Padding Mask ，用于屏蔽序列中的填充（PAD）位置\n",
    "    这里的q,k表示的是两个序列（跟注意力机制的q,k没有关系），\n",
    "    \n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    \n",
    "    返回 \n",
    "        pad_attn_mask :形状为[batch_size, len_q, len_k]的掩码, True表示被掩盖的位置\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()  # 这个seq_q只是用来expand维度的\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    # 例如:seq_k = [[1,2,3,4,0], [1,2,3,5,0]]\n",
    "    # [batch_size, 1, len_k], True is masked\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1) # 在某维插入新维度\n",
    "    # 还可以写作pad_attn_mask = (seq_k.data == 0).unsqueeze(1)\n",
    "    # [batch_size, len_q, len_k] 构成一个立方体(batch_size个这样的矩阵)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "\n",
    "\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    \"\"\"\n",
    "    seq: [batch_size, tgt_len]\n",
    "    \"\"\"\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # attn_shape: [batch_size, tgt_len, tgt_len]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)  # 生成一个上三角矩阵\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "    return subsequence_mask  # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "\n",
    "# ==========================================================================================\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        Q: Query [batch_size, n_heads, len_q, d_k]\n",
    "        K: Key [batch_size, n_heads, len_k, d_k]\n",
    "        V: Value [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        说明：在encoder-decoder的Attention层中len_q(q1,..qt)和len_k(k1,...km)可能不同\n",
    "        \"\"\"\n",
    "        \n",
    "        # 计算注意力分数并缩放\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)  \n",
    "        # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        \n",
    "        # mask矩阵填充scores（用-1e9填充scores中与attn_mask中值为1位置相对应的元素），使用掩码掩盖不需要的注意力\n",
    "        # Fills elements of self tensor with value where mask is True.\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "\n",
    "        attn = nn.Softmax(dim=-1)(scores)  # 对最后一个维度(v)做softmax\n",
    "        # scores : [batch_size, n_heads, len_q, len_k] * V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        # context: [batch_size, n_heads, len_q, d_v]\n",
    "        context = torch.matmul(attn, V)\n",
    "        # context：[[z1,z2,...],[...]]向量, attn注意力稀疏矩阵（用于可视化的）\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    实现多头注意力机制，用于三种场景：\n",
    "        Encoder的Self-Attention\n",
    "        Decoder的Masked Self-Attention\n",
    "        Encoder-Decoder的Attention\n",
    "    输入：seq_len x d_model\n",
    "    输出：seq_len x d_model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        实现多头注意力机制。\n",
    "\n",
    "        参数:\n",
    "        input_Q: query，形状为 [batch_size, len_q, d_model]\n",
    "        input_K: key，形状为 [batch_size, len_k, d_model]\n",
    "        input_V: value，形状为 [batch_size, len_v, d_model]\n",
    "        attn_mask: 注意力掩码，形状为 [batch_size, len_q, len_k]\n",
    "\n",
    "        返回:\n",
    "        output: 输出张量，形状为 [batch_size, len_q, d_model]\n",
    "        attn: 注意力权重\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)  # q,k必须维度相同，不然无法做点积\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        \n",
    "        # 多头注意力的输出需要与输入维度一致（不然怎么加residual呢）\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        \"\"\"\n",
    "        计算多头注意力的输出和注意力权重\n",
    "        input_Q: [batch_size, len_q, d_model]\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_V: [batch_size, len_v(=len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        \n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "        # 下面的多头的参数矩阵是放在一起做线性变换的，然后再拆成多个头，这是工程实现的技巧\n",
    "        # B: batch_size, S:seq_len, D: dim\n",
    "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, Head, W) -trans-> (B, Head, S, W)\n",
    "        #           线性变换               拆成多头\n",
    "\n",
    "        # 1. 将输入的Q、K、V投影到多头子空间\n",
    "        # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1,\n",
    "                                   n_heads, d_k).transpose(1, 2)\n",
    "        # K: [batch_size, n_heads, len_k, d_k] # K和V的长度一定相同，维度可以不同\n",
    "        K = self.W_K(input_K).view(batch_size, -1,\n",
    "                                   n_heads, d_k).transpose(1, 2)\n",
    "        # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        V = self.W_V(input_V).view(batch_size, -1,\n",
    "                                   n_heads, d_v).transpose(1, 2)\n",
    "\n",
    "        # 2. 扩展注意力掩码的维度以适应多头机制\n",
    "        # attn_mask: [batch_size, seq_len, seq_len] -> [batch_size, n_heads, seq_len, seq_len]\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "\n",
    "        # 3. 计算注意力\n",
    "        # context: [batch_size, n_heads, len_q, d_v]\n",
    "        # attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
    "        \n",
    "        # 4. 将不同头的输出向量拼接在一起\n",
    "        # context: [batch_size, n_heads, len_q, d_v] -> [batch_size, len_q, n_heads * d_v]\n",
    "        context = context.transpose(1, 2).reshape(\n",
    "            batch_size, -1, n_heads * d_v)\n",
    "\n",
    "        # 5. 使用全连接层，将维度映射会d_model\n",
    "        output = self.fc(context)  # [batch_size, len_q, d_model]\n",
    "        \n",
    "        # 6. 残差连接和Layer Norm\n",
    "        return nn.LayerNorm(d_model).to(device)(output + residual), attn\n",
    "\n",
    "\n",
    "# Pytorch中的Linear只会对最后一维操作，所以正好是我们希望的每个位置用同一个全连接网络\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        # [batch_size, seq_len, d_model]\n",
    "        return nn.LayerNorm(d_model).to(device)(output + residual)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # 这个是多头子注意力模块，用于处理 Encoder 输入的自身关系\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        # 逐位置的前馈网络，进一步处理多头注意力的输出\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        \"\"\"\n",
    "        enc_inputs: [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len]  mask矩阵(pad mask or sequence mask)\n",
    "        返回：\n",
    "            enc_outputs: [batch_size, src_len, d_model]\n",
    "            attn: [batch_size, n_heads, src_len, src_len]\n",
    "        \"\"\"\n",
    "        # 自注意力模块：\n",
    "        # 输入 enc_inputs 作为 Q, K, V（源序列内部的关系）\n",
    "        # 得到处理后的特征和注意力权重\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)  \n",
    "        \n",
    "        # 前馈网络\n",
    "        # 对多头注意力模块的输出进行逐位置的特征更新，保持形状不变\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        \n",
    "        # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # 自注意力模块\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        # 编码器-解码器注意力模块，用于关注编码器的输出特征\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        # 前馈网络，用于对特征进一步处理\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        \"\"\"\n",
    "        - dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        - enc_outputs: [batch_size, src_len, d_model]\n",
    "            编码器的输入序列，用作接码器的参考\n",
    "        - dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "            自注意力掩码，用来屏蔽未来时间步（防止泄漏未来信息）\n",
    "        - dec_enc_attn_mask: [batch_size, tgt_len, src_len]\n",
    "            编码器-解码器注意力的掩码，用于屏蔽编码器的填充位置\n",
    "            \n",
    "        返回：\n",
    "            解码器输出序列：[batch_size, tgt_len, d_model], \n",
    "            自注意力权重：[batch_size, n_heads, tgt_len, tgt_len]\n",
    "            编码器，解码器注意力权重\n",
    "        \"\"\"\n",
    "        # 1. 解码器自注意力机制\n",
    "        #   处理解码器自己的输入序列，掩盖未来时间步的信息（decoder需要按序生成）\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(\n",
    "            dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)  # 这里的Q,K,V全是Decoder自己的输入\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], \n",
    "        # dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "        # 2. 编码器-解码器注意力机制\n",
    "        #   解码器输出作为Q，编码器输出作为K和V用于解码器关注源序列的信息\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(\n",
    "            dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)  # Attention层的Q(来自decoder) 和 K,V(来自encoder)\n",
    "\n",
    "        # 3. 前馈网络\n",
    "        #   对解码器的输出序列逐位置地曾请特征表达能力\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "        # 返回解码器的输出以及两个注意力权重（用与可视化）\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
    "\n",
    "####################################################\n",
    "# 编码器的任务是将源序列映射到一个固定维度的表示\n",
    "#   Encoder的自注意力机制处理的是源序列的所有信息，允许每个位置都能够访问其他位置的信息\n",
    "#   由于没有生成序列的时间顺序问题，编码器的输入序列可以并行处理，且每个token之间的依赖关系可以是全局的\n",
    "#   编码器不用mask来演示信息流动，所有位置都彼此关注\n",
    "# 解码器的任务是根据编码器的输出（源序列的表示）生成目标序列\n",
    "#   Decoder的子注意力需要掩码，以确保模型不会看到未来时间步的内容\n",
    "######################################################\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # token Embedding 将输入的词汇表索引转换为词向量\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model) # 输入的词嵌入层，词汇表大小为src_vocab_size，d_model为词向量维度\n",
    "        # 位置编码（Transformer中位置编码时固定的，不需要学习）\n",
    "        self.pos_emb = PositionalEncoding(d_model) \n",
    "        # 定义多个EncoderLayer：每一层都由自注意力机制和前馈网络组成\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        \"\"\"\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        \"\"\"\n",
    "        enc_outputs = self.src_emb(enc_inputs)  # 词嵌入的结果：[batch_size, src_len, d_model]\n",
    "        \n",
    "        # 加上位置编码，位置编码提供每个词在序列中的位置信息\n",
    "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1)  # [batch_size, src_len, d_model]\n",
    "        \n",
    "        # Encoder输入序列的pad mask矩阵，避免自注意力计算的时候让padding部分参与计算\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  # [batch_size, src_len, src_len]\n",
    "        \n",
    "        # 博阿村每一层计算的注意力矩阵（可视化时使用，如画热力图来看各个词之间的关系）\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:  # 遍历每一层的Encoder Layer\n",
    "            # enc_outputs: [batch_size, src_len, d_model]\n",
    "            # enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
    "            # 传入的enc_outputs其实是input，传入mask矩阵是因为你要做self attention\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)  \n",
    "            enc_self_attns.append(enc_self_attn)  # 保存注意力矩阵做可视化\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 目标词嵌入层：将目标词汇索引转换为词向量\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)  # 目标词嵌入\n",
    "        # 位置编码：和Encoder是一样的，位置编码不需要学习\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        # 定义多个 DecoderLayerL每一层由自注意力机制和Encoder-Decoder注意力机制组成\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        \"\"\"\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        enc_outputs: [batch_size, src_len, d_model]   # 用在Encoder-Decoder Attention层\n",
    "        \"\"\"\n",
    "        dec_outputs = self.tgt_emb(dec_inputs)  # [batch_size, tgt_len, d_model]\n",
    "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).to(device)  # [batch_size, tgt_len, d_model]\n",
    "        \n",
    "        # Decoder输入序列的pad mask矩阵（这个例子中decoder是没有加pad的，实际应用中都是有pad填充的）\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).to(device)  # [batch_size, tgt_len, tgt_len]\n",
    "        \n",
    "        # Masked Self_Attention Mask：当前时刻是看不到未来的信息的\n",
    "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(device)  # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "        # Decoder中把两种mask矩阵相加（既屏蔽了padding，也屏蔽了未来时刻的信息）\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask),0).to(device)  \n",
    "        # [batch_size, tgt_len, tgt_len]; torch.gt比较两个矩阵的元素，大于则返回1，否则返回0\n",
    "\n",
    "        # 这个mask主要用于encoder-decoder attention层\n",
    "        # get_attn_pad_mask主要是enc_inputs的pad mask矩阵(因为enc是处理K,V的，求Attention时是用v1,v2,..vm去加权的，要把pad对应的v_i的相关系数设为0，这样注意力就不会关注pad向量)\n",
    "        #                       dec_inputs只是提供expand的size的\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)  # [batc_size, tgt_len, src_len]\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            # dec_outputs: [batch_size, tgt_len, d_model]\n",
    "            # dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
    "            # dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(\n",
    "                dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model]\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        # 初始化各部分组件\n",
    "        self.encoder = Encoder().to(device)\n",
    "        self.decoder = Decoder().to(device)\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).to(device)\n",
    "\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        \"\"\"\n",
    "        Transformers前向传播，接受源语言和目标语言序列作为输入\n",
    "        \n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        \"\"\"\n",
    "        # tensor to store decoder outputs\n",
    "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "\n",
    "        # enc_outputs: [batch_size, src_len, d_model]\n",
    "        # enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\n",
    "        # 经过Encoder网络后，得到的输出还是[batch_size, src_len, d_model]\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model]\n",
    "        # dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len]\n",
    "        # dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model] -> dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "        dec_logits = self.projection(dec_outputs) # 将解码器的输出从d_model映射到词汇表大小\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "\n",
    "model = Transformer().to(device)\n",
    "# 这里的损失函数里面设置了一个参数 ignore_index=0，因为 \"pad\" 这个单词的索引为 0，这样设置以后，就不会计算 \"pad\" 的损失（因为本来 \"pad\" 也没有意义，不需要计算）\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)  # 用adam的话效果不好\n",
    "\n",
    "# ====================================================================================================\n",
    "# 训练循环\n",
    "for epoch in range(epochs):\n",
    "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
    "        \"\"\"\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        dec_outputs: [batch_size, tgt_len]\n",
    "        \"\"\"\n",
    "        enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(\n",
    "            device), dec_inputs.to(device), dec_outputs.to(device)\n",
    "        # outputs: [batch_size * tgt_len, tgt_vocab_size]\n",
    "        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "        # dec_outputs.view(-1):[batch_size * tgt_len * tgt_vocab_size]\n",
    "        loss = criterion(outputs, dec_outputs.view(-1))\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def greedy_decoder(model, enc_input, start_symbol):\n",
    "    \"\"\"贪心编码\n",
    "    贪心解码是当K=1时的束搜索（Beam Search）。在推理阶段，由于我们不知道目标序列的输入，\n",
    "    因此我们通过逐步生成目标序列的每个单词，并将其逐步输入到Transformer模型中进行预测。\n",
    "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
    "    :param model: Transformer Model\n",
    "    :param enc_input: The encoder input\n",
    "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n",
    "    :return: The target input\n",
    "    \"\"\"\n",
    "    # 使用Encoder处理源语言输入，得到Encoder的输出以及自注意力权重\n",
    "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
    "    \n",
    "    # 初始化一个空的tensor: tensor([], size=(1, 0), dtype=torch.int64)\n",
    "    # 此张量表示解码器尚未生成任何单词\n",
    "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n",
    "    terminal = False # 终止标志\n",
    "    next_symbol = start_symbol # 下一个要生成的单词，初始化为起始符号\n",
    "    \n",
    "    while not terminal:\n",
    "        # 循环，直到生成结束符\n",
    "        # 预测阶段：dec_input序列会一点点变长（每次添加一个新预测出来的单词）\n",
    "        dec_input = torch.cat(\n",
    "            [dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)], -1)\n",
    "        # 使用解码器处理当前解码器输入和编码器输出，得到解码器的输出\n",
    "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
    "        # 使用Decoder的输出通过Proj层映射到目标词汇表大小，得到每个单词的概率分布\n",
    "        projected = model.projection(dec_outputs)\n",
    "        \n",
    "        # 获取当前时间步的预测概率（即最大值所在的位置，代表预测的单词）\n",
    "        # projected.squeeze(0): 移除batch维度，获取形状为[tgt_len, tgt_vocab_size]的张量\n",
    "        # max(dim=-1)是为了取出每个位置的最大概率，并返回其对应的索引\n",
    "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        \n",
    "        # 增量更新（我们希望重复单词预测结果是一样的）\n",
    "        # 我们在预测是会选择性忽略重复的预测的词，只摘取最新预测的单词拼接到输入序列中\n",
    "        # 拿出当前预测的单词(数字)。我们用x'_t对应的输出z_t去预测下一个单词的概率，不用z_1,z_2..z_{t-1}\n",
    "        next_word = prob.data[-1]\n",
    "        next_symbol = next_word\n",
    "        \n",
    "        if next_symbol == tgt_vocab[\"E\"]:\n",
    "            terminal = True\n",
    "        # print(next_word)\n",
    "    # greedy_dec_predict = torch.cat(\n",
    "    #     [dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],\n",
    "    #     -1)\n",
    "    # 返回解码器的预测序列，去掉了开始符号部分（即dec_input[:, 1:]）\n",
    "    # 因为解码器输入时已加上了起始符号，现在要去除起始符号\n",
    "    greedy_dec_predict = dec_input[:, 1:]\n",
    "    return greedy_dec_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "利用训练好的Transformer模型将中文句子'我 有 零 个 女 朋 友' 翻译成英文句子: \n",
      "tensor([1, 2, 8, 4, 9, 6, 7, 0]) -> tensor([ 1,  2,  6,  7,  5, 11])\n",
      "['我', '有', '零', '个', '女', '朋', '友', 'P'] -> ['I', 'have', 'zero', 'girl', 'friend', '.']\n"
     ]
    }
   ],
   "source": [
    "# 预测阶段\n",
    "# 测试集\n",
    "sentences = [\n",
    "    # enc_input                dec_input           dec_output\n",
    "    ['我 有 零 个 女 朋 友 P', '', ''],\n",
    "]\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
    "test_loader = Data.DataLoader(\n",
    "    MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n",
    "enc_inputs, _, _ = next(iter(test_loader))\n",
    "\n",
    "print()\n",
    "print(\"=\"*30)\n",
    "print(\"利用训练好的Transformer模型将中文句子'我 有 零 个 女 朋 友' 翻译成英文句子: \")\n",
    "for i in range(len(enc_inputs)):\n",
    "    greedy_dec_predict = greedy_decoder(model, enc_inputs[i].view(\n",
    "        1, -1).to(device), start_symbol=tgt_vocab[\"S\"])\n",
    "    print(enc_inputs[i], '->', greedy_dec_predict.squeeze())\n",
    "    print([src_idx2word[t.item()] for t in enc_inputs[i]], '->',\n",
    "          [idx2word[n.item()] for n in greedy_dec_predict.squeeze()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
