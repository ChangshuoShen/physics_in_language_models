{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搓完Transformer的结构之后开始手搓GPT2,自己写出来的结构之后方便做Probing\n",
    "\n",
    "计算 Transformer（以 GPT-2 为例）的参数量，需要将模型中的每个模块的参数逐一分解计算，然后汇总得到总参数量。\n",
    "\n",
    "假设有以下参数定义：\n",
    "- \\( n_{\\text{heads}} \\): 注意力头数\n",
    "- \\( n_{\\text{layers}} \\): Transformer 解码器层数\n",
    "- \\( d_{\\text{model}} \\): 每个 token 的嵌入维度（模型维度）\n",
    "- \\( d_{\\text{ffn}} \\): 前馈网络隐藏层的维度\n",
    "- \\( d_k \\): 每个注意力头的键和查询向量的维度\n",
    "- \\( d_v \\): 每个注意力头的值向量的维度\n",
    "- \\( V \\): 词汇表大小\n",
    "\n",
    "以下是分模块的参数计算公式和总参数量公式：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 输入嵌入和位置嵌入**\n",
    "- **词嵌入矩阵：**\n",
    "  \\[\n",
    "  \\text{Params}_{\\text{embeddings}} = V \\times d_{\\text{model}}\n",
    "  \\]\n",
    "  （词汇表大小 \\(V\\) × 模型维度 \\(d_{\\text{model}}\\)）\n",
    "\n",
    "- **位置嵌入矩阵：**\n",
    "  \\[\n",
    "  \\text{Params}_{\\text{positional embeddings}} = L \\times d_{\\text{model}}\n",
    "  \\]\n",
    "  （序列长度 \\(L\\) × 模型维度 \\(d_{\\text{model}}\\)）\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 注意力机制（Multi-Head Self-Attention, MHA）**\n",
    "对于单层的 MHA：\n",
    "1. **键 (Key)、查询 (Query)、值 (Value) 的投影层：**\n",
    "   - 每个头的键、查询、值权重：\\( d_{\\text{model}} \\times d_k, d_{\\text{model}} \\times d_k, d_{\\text{model}} \\times d_v \\)\n",
    "   - 总权重矩阵：\n",
    "     \\[\n",
    "     \\text{Params}_{\\text{QKV}} = 3 \\times d_{\\text{model}} \\times (d_k \\times n_{\\text{heads}})\n",
    "     \\]\n",
    "\n",
    "2. **注意力输出的线性层：**\n",
    "   - 汇聚后的多头注意力的投影：\n",
    "     \\[\n",
    "     \\text{Params}_{\\text{attention output}} = (d_k \\times n_{\\text{heads}}) \\times d_{\\text{model}}\n",
    "     \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 前馈网络 (Feed-Forward Network, FFN)**\n",
    "前馈网络由两层全连接层组成：\n",
    "1. 第一层的权重：\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{FFN1}} = d_{\\text{model}} \\times d_{\\text{ffn}}\n",
    "   \\]\n",
    "\n",
    "2. 第二层的权重：\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{FFN2}} = d_{\\text{ffn}} \\times d_{\\text{model}}\n",
    "   \\]\n",
    "\n",
    "3. 两层的偏置参数（可选，但通常较小）：\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{FFN bias}} = d_{\\text{ffn}} + d_{\\text{model}}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 层归一化 (Layer Normalization)**\n",
    "每层有两个参数（权重和偏置）：\n",
    "\\[\n",
    "\\text{Params}_{\\text{LayerNorm}} = 2 \\times d_{\\text{model}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **5. 输出层**\n",
    "输出层通常是一个线性层，将模型维度 \\(d_{\\text{model}}\\) 投影到词汇表大小 \\(V\\)：\n",
    "\\[\n",
    "\\text{Params}_{\\text{output}} = d_{\\text{model}} \\times V\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **总参数量公式**\n",
    "假设 Transformer 有 \\( n_{\\text{layers}} \\) 层，则总参数量公式为：\n",
    "\n",
    "\\[\n",
    "\\text{Total Parameters} =\n",
    "\\text{Params}_{\\text{embeddings}} + \\text{Params}_{\\text{positional embeddings}} + n_{\\text{layers}} \\times \\left( \\text{Params}_{\\text{MHA}} + \\text{Params}_{\\text{FFN}} + \\text{Params}_{\\text{LayerNorm}} \\right) + \\text{Params}_{\\text{output}}\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "- **MHA 参数：**\n",
    "  \\[\n",
    "  \\text{Params}_{\\text{MHA}} = \\text{Params}_{\\text{QKV}} + \\text{Params}_{\\text{attention output}}\n",
    "  = 3 \\times d_{\\text{model}} \\times (d_k \\times n_{\\text{heads}}) + (d_k \\times n_{\\text{heads}}) \\times d_{\\text{model}}\n",
    "  \\]\n",
    "\n",
    "- **FFN 参数：**\n",
    "  \\[\n",
    "  \\text{Params}_{\\text{FFN}} = \\text{Params}_{\\text{FFN1}} + \\text{Params}_{\\text{FFN2}} + \\text{Params}_{\\text{FFN bias}}\n",
    "  = d_{\\text{model}} \\times d_{\\text{ffn}} + d_{\\text{ffn}} \\times d_{\\text{model}} + d_{\\text{ffn}} + d_{\\text{model}}\n",
    "  \\]\n",
    "\n",
    "- **LayerNorm 参数：**\n",
    "  \\[\n",
    "  \\text{Params}_{\\text{LayerNorm}} = 2 \\times d_{\\text{model}}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **参数量示例**\n",
    "假设：\n",
    "- \\( n_{\\text{heads}} = 12 \\)\n",
    "- \\( n_{\\text{layers}} = 12 \\)\n",
    "- \\( d_{\\text{model}} = 768 \\)\n",
    "- \\( d_{\\text{ffn}} = 3072 \\)\n",
    "- \\( d_k = d_v = d_{\\text{model}} / n_{\\text{heads}} = 64 \\)\n",
    "- \\( V = 50,000 \\)\n",
    "- \\( L = 512 \\)\n",
    "\n",
    "计算：\n",
    "1. **嵌入层：**\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{embeddings}} = 50,000 \\times 768 = 38.4M\n",
    "   \\]\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{positional embeddings}} = 512 \\times 768 = 0.39M\n",
    "   \\]\n",
    "\n",
    "2. **MHA（单层）：**\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{QKV}} = 3 \\times 768 \\times 64 \\times 12 = 1.77M\n",
    "   \\]\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{attention output}} = 768 \\times 768 = 0.59M\n",
    "   \\]\n",
    "   总计：\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{MHA}} = 1.77M + 0.59M = 2.36M\n",
    "   \\]\n",
    "\n",
    "3. **FFN（单层）：**\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{FFN}} = 768 \\times 3072 + 3072 \\times 768 + 3072 + 768 = 4.72M\n",
    "   \\]\n",
    "\n",
    "4. **LayerNorm（单层）：**\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{LayerNorm}} = 2 \\times 768 = 1.5K\n",
    "   \\]\n",
    "\n",
    "5. **每层参数：**\n",
    "   \\[\n",
    "   \\text{Params}_{\\text{per layer}} = \\text{Params}_{\\text{MHA}} + \\text{Params}_{\\text{FFN}} + \\text{Params}_{\\text{LayerNorm}} \\approx 7.08M\n",
    "   \\]\n",
    "\n",
    "6. **总参数量：**\n",
    "   \\[\n",
    "   \\text{Total Parameters} = 38.4M + 0.39M + 12 \\times 7.08M + 50,000 \\times 768\n",
    "   = 124M\n",
    "   \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# ====================================================================================================\n",
    "# 位置编码类 (Positional Encoding)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        '''\n",
    "        初始化位置编码类\n",
    "        d_model:    模型的维度，即每个词的嵌入维度\n",
    "        dropout:    防止过拟合的dropout\n",
    "        max_len:    最大序列长度，用于生成位置编码\n",
    "        '''\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # 用于存储位置编码\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # 偶数维度使用正弦，奇数维使用余弦\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 添加一个batch维度并转置\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        # 将位置编码存储为不可训练的缓冲区\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        将位置编码添加到张量x中\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        加入位置编码并dropout之后的张量\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    \"\"\"\n",
    "    创建 Padding Mask ，用于屏蔽序列中的填充（PAD）位置\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    返回 \n",
    "        pad_attn_mask :形状为[batch_size, len_q, len_k]的掩码, True表示被掩盖的位置\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()  # 这个seq_q只是用来expand维度的\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # 在某维插入新维度\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "\n",
    "\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    \"\"\"\n",
    "    seq: [batch_size, tgt_len]\n",
    "    \"\"\"\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)  # 生成一个上三角矩阵\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "    return subsequence_mask  # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "\n",
    "# ==========================================================================================\n",
    "# 缩放点积注意力（Scaled Dot Product Attention）\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        Q: Query [batch_size, n_heads, len_q, d_k]\n",
    "        K: Key [batch_size, n_heads, len_k, d_k]\n",
    "        V: Value [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1))  # 点积并缩放\n",
    "        scores.masked_fill_(attn_mask, -1e9)  # 使用掩码屏蔽掉填充位置\n",
    "\n",
    "        attn = nn.Softmax(dim=-1)(scores)  # 对最后一个维度做softmax\n",
    "        context = torch.matmul(attn, V)  # 计算上下文向量\n",
    "\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "# 多头注意力机制 (Multi-head Attention)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 线性层将输入映射到多头子空间\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        \n",
    "        # 用于将多头注意力的输出映射回d_model\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        batch_size = input_Q.size(0)\n",
    "\n",
    "        # 将输入的Q、K、V投影到多个子空间\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(input_K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(input_V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
    "\n",
    "        # 扩展注意力掩码的维度以适应多头机制\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
    "\n",
    "        # 计算缩放点积注意力\n",
    "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
    "\n",
    "        # 将不同头的输出拼接在一起\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1, self.n_heads * self.d_v)\n",
    "\n",
    "        # 使用全连接层将拼接的输出映射到模型维度\n",
    "        output = self.fc(context)\n",
    "\n",
    "        return nn.LayerNorm(self.d_model)(output + input_Q), attn\n",
    "\n",
    "\n",
    "# 前馈神经网络 (Feed-Forward Network)\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        return nn.LayerNorm(inputs.size(-1))(output + residual)\n",
    "\n",
    "\n",
    "# 编码器层 (Encoder Layer)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, n_heads, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs, attn\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
